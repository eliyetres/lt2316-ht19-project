{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the folder containing the .story files\n",
    "data_path = \"\"\n",
    "# name of the files. the data will be split and saved as the selected name + _train, _test...\n",
    "save_file = \"\" \n",
    "stop_words = set(stopwords.words('english'))\n",
    "# set the max number of tokens for summaries and highlights\n",
    "stories_maxlen = 400\n",
    "highlights_maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the files inside a directory\n",
    "def load_doc(filename):\n",
    "    \"\"\" Load stories to memory \"\"\"\n",
    "    # open the file as read only\n",
    "    file = open(filename, encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def split_story(doc):\n",
    "    \"\"\" Split a document into news story and highlights \"\"\"\n",
    "    # find first highlight\n",
    "    index = doc.find('@highlight')\n",
    "    # split into story and highlights\n",
    "    story, highlights = doc[:index], doc[index:].split('@highlight')\n",
    "    # strip extra white space around each highlight\n",
    "    highlights = [h.strip() for h in highlights if len(h) > 0]\n",
    "    return story, highlights\n",
    "\n",
    "def load_stories(directory):\n",
    "    \"\"\" Load all stories in a directory \"\"\"\n",
    "    stories = []\n",
    "    for name in listdir(directory):\n",
    "        filename = directory + '/' + name\n",
    "        # load document\n",
    "        doc = load_doc(filename)\n",
    "        # split into story and highlights\n",
    "        story, highlights = split_story(doc)\n",
    "        # store\n",
    "        stories.append({'story': story, 'highlights': highlights})\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_prefixes(word):\n",
    "    contractions = { \n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\"\n",
    "    }\n",
    "    if word in contractions.keys():\n",
    "        word = contractions[word]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lines(lines):\n",
    "    \"\"\" Clean list with all lines \"\"\"\n",
    "    cleaned = \"\"\n",
    "    for line in lines:\n",
    "        # strip source cnn office if it exists\n",
    "        index = line.find('(CNN) -- ')\n",
    "        if index > -1:\n",
    "            line = line[index+len('(CNN)'):]\n",
    "        # remove CNN titles\n",
    "        line = line.replace('(CNN)', '')\n",
    "        # remove this line, not\n",
    "        if \"contributed to this report\" in line:\n",
    "            line = \" \"\n",
    "\n",
    "        # replace - and / with space to avoid compund words\n",
    "        line = line.replace('-', ' ').replace('/', ' ')\n",
    "        # remove wierd characters\n",
    "        line = re.sub(r'[\\?\\!\\\"\\*\\&\\:\\.\\,\\(\\)\\$\\;\\»Ã©Ã±\\@\\#\\%•\\+\\]', '', line)\n",
    "        # tokenize on white space\n",
    "        line = line.split()\n",
    "        # convert to lower case\n",
    "        line = [word.lower() for word in line]\n",
    "        # replace word contractions will full words\n",
    "        line = [change_contractions(word) for word in line]\n",
    "        # remove empty lines\n",
    "        line = [c for c in line if len(c) > 0]\n",
    "        line = \" \".join(line)\n",
    "        # remove aphostrophes (after changing contractions)\n",
    "        line = re.sub(r'(\\')', '', line)\n",
    "        # remove more than 2 whitespaces\n",
    "        line = re.sub(r'[ ]{2,}', ' ', line)\n",
    "        if not len(line) == 0:\n",
    "            cleaned = cleaned+line+\" . \"\n",
    "\n",
    "    cleaned = cleaned[:-3]\n",
    "    #cleaned = re.sub(r'( \\. )', ' <e> <s> ', cleaned)\n",
    "    cleaned = re.sub(r'( \\. )', ' ', cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_stories(cleaned, max_len):\n",
    "    cleaned = cleaned.split(\" \")\n",
    "    # remove the stopwords from the texts because they do not provide much use for training the model.\n",
    "    # keep them for the summaries so that they sound more like natural phrases\n",
    "    cleaned = [c for c in cleaned if not c in stop_words]\n",
    "    cleaned = cleaned[0:max_len]\n",
    "    cleaned = \" \".join(cleaned)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_highlights(cleaned, max_len):\n",
    "    cleaned = cleaned.split(\" \")\n",
    "    cleaned = cleaned[0:max_len]\n",
    "    cleaned = \" \".join(cleaned)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(stories, size=0.20):\n",
    "    # Splitting data into sets\n",
    "    trainset, testset = train_test_split(stories, test_size=size)\n",
    "    # Printing the lengths of the sets\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stories(data_path, save_file):\n",
    "    print(\"Loading stories...\")\n",
    "    directory = data_path\n",
    "    stories = load_stories(directory)\n",
    "    print(\"Loaded number of stories: {}.\".format(len(stories)))\n",
    "    \n",
    "    # clean stories\n",
    "    for example in stories:\n",
    "        example[\"story\"] = clean_lines(example['story'].split('\\n'))\n",
    "        example[\"highlights\"] = clean_lines(example[\"highlights\"])\n",
    "        example[\"story\"] = cut_stories(example['story'], 400)\n",
    "        example[\"highlights\"] = cut_highlights(example[\"highlights\"], 100)\n",
    "        if example[\"story\"] == \"\":\n",
    "            example[\"story\"] = np.nan # way of dropping empty lines from pd object later\n",
    "\n",
    "    # split text\n",
    "    print(\"Splitting stories into train and test...\")\n",
    "    train, test = split_data(stories)\n",
    "    train, val = split_data(train)\n",
    "    print(\"Number of stories in training set: {}\".format(len(train)))\n",
    "    print(\"Number of stories in validation set: {}\".format(len(val)))\n",
    "    print(\"Number of stories in test set: {}\".format(len(test)))\n",
    "\n",
    "    print(\"Writing data to files...\")\n",
    "    test_filename = save_file + \"_train.csv\"\n",
    "    val_filename = save_file + \"_val.csv\"\n",
    "    train_filename = save_file + \"_test.csv\"\n",
    "\n",
    "    df1 = pd.DataFrame.from_dict(train)\n",
    "    df2 = pd.DataFrame.from_dict(test)\n",
    "    df3 = pd.DataFrame.from_dict(val) \n",
    "    \n",
    "    # Drop rows with any empty cells\n",
    "    df1.dropna(how='any', inplace=True)  \n",
    "    df2.dropna(how='any', inplace=True)   \n",
    "    df3.dropna(how='any', inplace=True)\n",
    "\n",
    "    # write to file\n",
    "    df3.to_csv(val_filename, encoding='utf-8', index=False)\n",
    "    df1.to_csv(train_filename, encoding='utf-8', index=False)\n",
    "    df2.to_csv(test_filename, encoding='utf-8', index=False)\n",
    "\n",
    "    print(\"Finished processing data, saved train, validation and test files as {}, {} and {} \".format(\n",
    "        train_filename, val_filename, test_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_stories(data_path, save_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
